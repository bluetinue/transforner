{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9/3Eb4kAjlO6BHgXAq3lj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bluetinue/transforner/blob/main/%E5%8F%82%E8%80%83%E4%BB%A3%E7%A0%81_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qLEyWCkZweK-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# todo:1.定义WordEmbedding\n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        # vocab_size:代表单词的总个数\n",
        "        self.vocab_size = vocab_size\n",
        "        # embed_dim:代表词嵌入维度\n",
        "        self.embed_dim = embed_dim\n",
        "        # 定义Embedding层\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x--》[batch_size, seq_len]\n",
        "        return self.embed(x) * math.sqrt(self.embed_dim)\n"
      ],
      "metadata": {
        "id": "uy9stKs-wwWD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# todo:2.定义位置编码模型PositionEncoding\n",
        "class PositionEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout_p, max_len=60):\n",
        "        super().__init__()\n",
        "        # d_model:代表词嵌入维度\n",
        "        self.d_model = d_model\n",
        "        # dropout_p:代表随机失活的系数\n",
        "        self.dropout_p = dropout_p\n",
        "        # max_len:代表最大句子长度\n",
        "        self.max_len = max_len\n",
        "        # 定义dropout层\n",
        "        self.dropout = nn.Dropout(p=dropout_p)\n",
        "        # 根据三角函数的公式实现位置的编码\n",
        "        # 定义位置编码矩阵[max_len, d_model]-->[60, 512]\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        # 定义位置列矩阵--》[max_len, 1]-->[60, 1]\n",
        "        position = torch.arange(0, max_len).unsqueeze(dim=1)\n",
        "        # 定义转换矩阵：根据三角函数的计算公式，是其中的除了pos之外的系数（频率）\n",
        "        # temp_vec-->[256]\n",
        "        temp_vec = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000)/d_model))\n",
        "        # 根据三角函数的计算公式，计算角度:pos_vec-->[60, 256]\n",
        "        pos_vec = position * temp_vec\n",
        "        # 将奇数位用sin处理，偶数位用cos处理\n",
        "        pe[:, 0::2] = torch.sin(pos_vec)\n",
        "        pe[:, 1::2] = torch.cos(pos_vec)\n",
        "        # 需要对上述的位置编码结果升维:pe-->[1, max_len, d_model]-->[1, 60, 512]\n",
        "        pe = pe.unsqueeze(dim=0)\n",
        "        # pe位置编码结果不随着模型的训练而更新，因此需要进行注册到缓存区\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x--》来自于embedding之后的结果--》[batch_size, seq_len, embed_dim]-->[2, 4, 512]\n",
        "        # 将x和位置编码的信息进行融合\n",
        "        x = x + self.pe[:, :x.size()[1]]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "h2i-e3t7wxEz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# todo:1. 定义attention计算的方法\n",
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    # query/key/value-->[batch_size, seq_len, embed_dim]\n",
        "    # mask-->shape-->[batch_size, seq_len, seq_len]\n",
        "    # dropout--》实例化的对象\n",
        "    # 第一步：获得词嵌入表达的维度\n",
        "    d_k = query.size(-1)\n",
        "    # 第二步：计算query和key之间的相似性分数（注意力权重分数（未经过softmax归一化的结果））\n",
        "    # query-->[2, 4, 512];key-->[2, 4, 512]-->转置--》[2, 512,4]. 相乘后--》scores-->[2, 4, 4]\n",
        "    scores = torch.matmul(query, torch.transpose(key, -1, -2)) / math.sqrt(d_k)\n",
        "    # 第三步：判断是否需要mask\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask==0, -1e9)\n",
        "    # print(f'未归一化的scores--》{scores}')\n",
        "    # 第四步：进行softmax归一化\n",
        "    atten_weights = F.softmax(scores, dim=-1)\n",
        "    # print(f'atten_weights--》{atten_weights}')\n",
        "    # 第五步：如果有dropout 就进行随机失活防止过拟合\n",
        "    if dropout is not None:\n",
        "        atten_weights = dropout(atten_weights)\n",
        "\n",
        "    return torch.matmul(atten_weights, value), atten_weights\n"
      ],
      "metadata": {
        "id": "asYMP-CMw8_F"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "metadata": {
        "id": "ZZ6HROUSw9vB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# todo:2. 定义多头注意力类\n",
        "class MutiHeadAttention(nn.Module):\n",
        "    def __init__(self, head, embed_dim, dropout_p=0.1):\n",
        "        super().__init__()\n",
        "        # 第一步：确定embed_dim是否能被head整除\n",
        "        assert embed_dim % head == 0\n",
        "        # 第二步：确定每个head应该处理多少维度特征\n",
        "        self.d_k = embed_dim // head\n",
        "        # 第三步：定义head的属性\n",
        "        self.head = head\n",
        "        # 第四步：定义4个全连接层\n",
        "        self.linears = clones(nn.Linear(embed_dim, embed_dim), 4)\n",
        "        # 第五步：定义atten权重属性\n",
        "        self.atten = None\n",
        "        # 第六步：实例化dropout对象\n",
        "        self.dropout = nn.Dropout(p=dropout_p)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # 需要对mask的形状进行升维度\n",
        "        # mask-->输入的形状--》[head, seq_len, seq_len]-->[8, 4, 4],升维之后--》[1, 8, 4, 4]\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(dim=0)\n",
        "        # 获取当前输入的batch_size\n",
        "        batch_size = query.size(0)\n",
        "        # 开始处理query，key，value，都要经过线性变化并且切分为8个头\n",
        "        # model(x)-->就是将数据经过linear层处理x-->[2, 4, 512]-->经过Linear-->[2, 4, 512]-->分割--》[2, 4, 8, 64]-->transpose-->[2, 8, 4, 64]\n",
        "        # query，key，value--》shape-->[2, 8, 4, 64]\n",
        "        query, key, value = [model(x).view(batch_size, -1, self.head, self.d_k).transpose(1, 2)\n",
        "                             for model, x in zip(self.linears, (query, key, value))]\n",
        "        # 接下来将上述处理后的query，key，value--》shape-->[2, 8, 4, 64]送入attention方法进行注意力的计算:\n",
        "        # query--》[2, 8, 4, 64]和key--》[2, 8, 4, 64]转置结果[2, 8, 64, 4]进行相乘--》shape--》[2,8, 4, 4](所以传的mask矩阵是4维的)\n",
        "        # [2, 8, 4, 4]要和value-->[2, 8, 4, 64]-->相乘--》shape--》x-->[2, 8, 4, 64]\n",
        "        x, self.atten = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
        "        # 需要将多头注意力的结果进行合并\n",
        "        #  x.transpose(1, 2)-->【2,4, 8, 64】\n",
        "        # y 合并后的结果-->[2, 4, 512]\n",
        "        y = x.transpose(1, 2).contiguous().view(batch_size, -1, self.head*self.d_k)\n",
        "        # 经过线性变化得到指定输出维度的结果\n",
        "        return self.linears[-1](y)"
      ],
      "metadata": {
        "id": "DiE1Gj3Yw_k1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# todo:3. 定义前馈全连接层：两层线性层\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout_p=0.1):\n",
        "        super().__init__()\n",
        "        # d_model:第一个全连接层输入的特征维度；第二个全连接层输出的特征维度\n",
        "        self.d_model = d_model\n",
        "        # d_ff: 第一个全连接层输出的特征维度；第二个全连接层输入的特征维度\n",
        "        self.d_ff = d_ff\n",
        "        # 定义第一个全连接层\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        # 定义第二个全连接层\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        # 定义dropout层\n",
        "        self.dropout = nn.Dropout(p=dropout_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))"
      ],
      "metadata": {
        "id": "H8JelByLxCBE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# todo:4. 定义规范化层：让数据符合标准正态分布\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super().__init__()\n",
        "        # 定义属性\n",
        "        self.features = features # 代表词嵌入维度\n",
        "        # eps\n",
        "        self.eps = eps\n",
        "        # 定义一个模型的参数（系数）\n",
        "        self.a = nn.Parameter(torch.ones(features))\n",
        "        self.b = nn.Parameter(torch.zeros(features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x--->[2, 4, 512]\n",
        "        # 1.求出均值:x_mean-->[2, 4, 1]\n",
        "        x_mean = torch.mean(x, dim=-1, keepdim=True)\n",
        "        # 2.求出标准差\n",
        "        x_std = torch.std(x, dim=-1, keepdim=True)\n",
        "        return self.a * (x - x_mean) / (x_std + self.eps) + self.b"
      ],
      "metadata": {
        "id": "5yEd4QjHxEEi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# todo:5. 定义子层连接结构\n",
        "class SublayerConnection(nn.Module):\n",
        "    def __init__(self, size, dropout_p=0.1):\n",
        "        super().__init__()\n",
        "        # 定义size属性：词嵌入的维度大小\n",
        "        self.size = size\n",
        "        # 实例化规范化层\n",
        "        self.layer_norm = LayerNorm(features=size)\n",
        "        # 实例化dropout层\n",
        "        self.dropout = nn.Dropout(p=dropout_p)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        # x--》来自于输入部分：positionEncoding+WordEmbedding;[batch_size, seq_len, embed_dim]-->[2, 4, 512]\n",
        "        # sublayer-->代表函数的对象：可以是处理多头自注意力机制函数的对象，也可以是前馈全连接层对象\n",
        "        # post_norm\n",
        "        x1 = x + self.dropout(self.layer_norm(sublayer(x)))\n",
        "        # pre_norm\n",
        "        # x1 = x + self.dropout(sublayer(self.layer_norm(x)))\n",
        "        return x1\n"
      ],
      "metadata": {
        "id": "6l1S9O7sxHOz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# todo:6. 定义编码器层\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, size, self_atten, feed_forward, dropout_p):\n",
        "        super().__init__()\n",
        "        # size:代表词嵌入的维度\n",
        "        self.size = size\n",
        "        # self_atten:代表多头自注意力机制的对象\n",
        "        self.self_atten = self_atten\n",
        "        # feed_forward:代表前馈全连接层的对象\n",
        "        self.feed_forward = feed_forward\n",
        "        # 定义两层子层连接结构\n",
        "        self.sub_layers = clones(SublayerConnection(size, dropout_p), 2)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # x-->来自输入部分--》[batch_size, seq_len, embed_dim]:[2, 4, 512]\n",
        "        # mask-->[head, seq_len, seq_len]-=-->[8, 4, 4]\n",
        "        # 经过第一个子层连接结构：先经过多头自注意力层--》然后经过norm-->最后残差连接\n",
        "        x1 = self.sub_layers[0](x, lambda x: self.self_atten(x, x, x, mask))\n",
        "        # 经过第二个子层连接结构：先经过前馈全连接层--》然后经过norm-->最后残差连接\n",
        "        x2 = self.sub_layers[1](x1, self.feed_forward)\n",
        "        return x2"
      ],
      "metadata": {
        "id": "yZ2oqOoIxKbc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# todo:7. 定义编码器\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, layer, N):\n",
        "        super().__init__()\n",
        "        # layer：代表编码器层\n",
        "        self.layer = layer\n",
        "        # N:代表有几个编码器层\n",
        "        # 定义N个编码器层\n",
        "        self.layers = clones(layer, N)\n",
        "        # 实例化规范化层\n",
        "        self.norm = LayerNorm(features=layer.size)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # x-->来自输入部分--》[batch_size, seq_len, embed_dim]:[2, 4, 512]\n",
        "        # mask-->[head, seq_len, seq_len]-=-->[8, 4, 4]\n",
        "        # for循环迭代N个编码器层得到最终的结果\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "99Qd1fwBxNaV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# todo:1. 定义解码器层\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, size, self_atten, src_atten, feed_forward, dropout_p):\n",
        "        super().__init__()\n",
        "        # size:代表词嵌入维度的大小\n",
        "        self.size = size\n",
        "        # self_atten:自注意力机制的对象：Q=K=V\n",
        "        self.self_atten = self_atten\n",
        "        # src_atten:一般注意力机制的对象：Q!=K=V\n",
        "        self.src_atten = src_atten\n",
        "        # feed_forward:前馈全连接层对象\n",
        "        self.feed_forward = feed_forward\n",
        "\n",
        "        # 定义三个子层连接结构\n",
        "        self.sub_layers = clones(SublayerConnection(size, dropout_p), 3)\n",
        "\n",
        "    def forward(self, y, encoder_output, source_mask, target_mask):\n",
        "        # y:代表解码器的输入--》[batch_size, seq_len, embed_dim]\n",
        "        # encoder_output:代表编码器的输出结果--》[batch_size, seq_len, emebed_dim]\n",
        "        # target_mask防止未来信息被提前看到/target_mask-->[head, y_seq_len, y_seq_len]\n",
        "        # source_mask消除padding的影响# source_mask--shape-->[head, y_seq_len, x_seq_len]\n",
        "        # 经过第一个子层连接结构\n",
        "        y1 = self.sub_layers[0](y, lambda x: self.self_atten(x, x, x, target_mask))\n",
        "        # 经过第二个子层连接结构\n",
        "        # query--》[2,6,512]-->[2, 8, 6, 64],key/value-->[2, 4, 512]-->[2, 8, 4, 64]\n",
        "        # [2, 8, 6, 64]--和[2, 8, 4, 64]转置[2,8, 64, 4]-->[2, 8, 6, 4]\n",
        "        y2 = self.sub_layers[1](y1, lambda x: self.src_atten(x, encoder_output, encoder_output, source_mask))\n",
        "        # 经过第三个子层连接结构\n",
        "        y3 = self.sub_layers[2](y2, self.feed_forward)\n",
        "        return y3"
      ],
      "metadata": {
        "id": "NAXt3p39xP8F"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#todo:2.定义解码器\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, layer, N):\n",
        "        super().__init__()\n",
        "        # layer：代表解码器层\n",
        "        self.layer = layer\n",
        "        # N:代表有几个解码器层\n",
        "        # 定义N个解码层\n",
        "        self.layers = clones(layer, N)\n",
        "        # 实例化规范化层\n",
        "        self.norm = LayerNorm(features=layer.size)\n",
        "\n",
        "    def forward(self, y, encoder_output, source_mask, target_mask):\n",
        "        # y:代表解码器的输入--》[batch_size, seq_len, embed_dim]\n",
        "        # encoder_output:代表编码器的输出结果--》[batch_size, seq_len, emebed_dim]\n",
        "        # target_mask防止未来信息被提前看到/target_mask-->[head, y_seq_len, y_seq_len]\n",
        "        # source_mask消除padding的影响# source_mask--shape-->[head, y_seq_len, x_seq_len]\n",
        "        # for循环迭代N个编码器层得到最终的结果\n",
        "        for layer in self.layers:\n",
        "            y = layer(y, encoder_output, source_mask, target_mask)\n",
        "        return self.norm(y)"
      ],
      "metadata": {
        "id": "3CrIhVxKxUkl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#输出部分\n",
        "class Output(nn.Module):\n",
        "  def __init__(self,d_model,vocab_size):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(d_model,vocab_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return F.log_softmax(self.linear(x),dim=-1)"
      ],
      "metadata": {
        "id": "HjKzV6wQ7aJ-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "vocab_size = 10000\n",
        "output = Output(d_model,vocab_size)\n",
        "\n",
        "x = torch.randn(2,6,512)\n",
        "y = output(x)\n",
        "print(y)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sow4548C9Hq1",
        "outputId": "3c31e8c4-0406-453d-f9c5-aef004284a4d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ -9.7111,  -8.1889,  -8.9062,  ...,  -8.6487,  -9.8830,  -9.3482],\n",
            "         [ -8.6914,  -8.5643,  -9.8532,  ..., -10.1607,  -8.9868,  -9.0823],\n",
            "         [-10.6884,  -9.0723,  -9.5504,  ...,  -9.4564, -10.1345,  -9.2651],\n",
            "         [ -9.5655,  -9.3086,  -9.1617,  ...,  -9.9297, -10.0550,  -8.7121],\n",
            "         [ -8.7577,  -9.6381,  -9.0351,  ...,  -9.3632, -10.1828,  -9.0489],\n",
            "         [ -9.7267, -10.1449,  -9.1718,  ...,  -8.6863,  -9.1839,  -9.9713]],\n",
            "\n",
            "        [[ -8.9630,  -7.8263,  -9.5879,  ...,  -8.8451, -10.6653,  -9.1960],\n",
            "         [ -8.7935,  -9.0358,  -9.1744,  ...,  -9.8542, -11.0156, -10.0952],\n",
            "         [ -9.8142,  -9.8237, -10.1341,  ...,  -9.9889,  -9.0870,  -9.4520],\n",
            "         [ -9.4690,  -8.9369,  -9.8516,  ...,  -9.8868,  -9.5688, -10.1899],\n",
            "         [ -8.8104,  -8.5893,  -8.6542,  ...,  -9.0583,  -9.5103,  -9.5428],\n",
            "         [ -9.3941,  -9.3329,  -8.3968,  ...,  -9.2477,  -9.0286,  -9.2541]]],\n",
            "       grad_fn=<LogSoftmaxBackward0>)\n",
            "torch.Size([2, 6, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#定义Encoder2Decoder类\n",
        "class Encoder2Decoder(nn.Module):\n",
        "  def __init__(self,encoder,decoder,source_embed,target_embed,Output):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.source_embed = source_embed\n",
        "    self.target_embed = target_embed\n",
        "    self.output = output\n",
        "  def forward(self,source,target,source_mask1,source_mask2,target_mask):\n",
        "    encoder_word_embed = self.source_embed(source)\n",
        "    encoder_output = self.encoder(encoder_word_embed,source_mask1)\n",
        "    decoder_word_embed = self.target_embed(target)\n",
        "    decoder_output = self.decoder(decoder_word_embed,encoder_output,source_mask2,target_mask)\n",
        "    output = self.output(decoder_output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "lMfWdVeI9rf4"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#构建transformer模型\n",
        "def mk_model():\n",
        "  #实例化编码器对象\n",
        "  MutiHead = MutiHeadAttention(embed_dim=512,head=8,dropout_p=0.1)\n",
        "  feed_forward = FeedForward(512,2048)\n",
        "  encoder_layer = EncoderLayer(512,MutiHead,feed_forward,0.1)\n",
        "  encoder = Encoder(encoder_layer,6)\n",
        "\n",
        "  #实例化解码器对象\n",
        "  self_attn = copy.deepcopy(MutiHead)\n",
        "  src_attn = copy.deepcopy(MutiHead)\n",
        "  ff = copy.deepcopy(feed_forward)\n",
        "  decoder_layer = DecoderLayer(512,self_attn,src_attn,ff,0.1)\n",
        "  decoder = Decoder(decoder_layer,6)\n",
        "\n",
        "  #源语言经过wordEmbedding--》positionEncoding\n",
        "  vocab_size = 1000\n",
        "  d_model = 512\n",
        "  encoder_embed = Embeddings(vocab_size=vocab_size,embed_dim=d_model)\n",
        "  encoder_pos = PositionEncoding(d_model,0.1)\n",
        "  #输入\n",
        "  source_embed = nn.Sequential(encoder_embed,encoder_pos)\n",
        "\n",
        "  decoder_embed = copy.deepcopy(encoder_embed)\n",
        "  decoder_pos = copy.deepcopy(encoder_pos)\n",
        "  target_embed = nn.Sequential(decoder_embed,decoder_pos)\n",
        "\n",
        "  output = Output(d_model,vocab_size)\n",
        "  #实例化model\n",
        "  model = Encoder2Decoder(encoder,decoder,source_embed,target_embed,output)\n",
        "  print(model)\n",
        "  #准备数据\n",
        "  source = torch.randint(0, vocab_size, (2, 4))\n",
        "  target = torch.randint(0, vocab_size, (2, 6))\n",
        "  print(source)\n",
        "  print(target)\n",
        "\n",
        "  source_mask1 = torch.zeros(8,4,4)\n",
        "  source_mask2 = torch.zeros(8,6,4)\n",
        "  target_mask = torch.zeros(8,6,6)\n",
        "\n",
        "  result =model(source,target,source_mask1,source_mask2,target_mask)\n",
        "  print(result)"
      ],
      "metadata": {
        "id": "dOYYrDplPFGv"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mk_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HujqqusWRgRw",
        "outputId": "d6cc8564-9e31-4921-c5bb-430e55f1bb73"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder2Decoder(\n",
            "  (encoder): Encoder(\n",
            "    (layer): EncoderLayer(\n",
            "      (self_atten): MutiHeadAttention(\n",
            "        (linears): ModuleList(\n",
            "          (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (sub_layers): ModuleList(\n",
            "        (0-1): 2 x SublayerConnection(\n",
            "          (layer_norm): LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x EncoderLayer(\n",
            "        (self_atten): MutiHeadAttention(\n",
            "          (linears): ModuleList(\n",
            "            (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): FeedForward(\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (sub_layers): ModuleList(\n",
            "          (0-1): 2 x SublayerConnection(\n",
            "            (layer_norm): LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm()\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (layer): DecoderLayer(\n",
            "      (self_atten): MutiHeadAttention(\n",
            "        (linears): ModuleList(\n",
            "          (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (src_atten): MutiHeadAttention(\n",
            "        (linears): ModuleList(\n",
            "          (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (sub_layers): ModuleList(\n",
            "        (0-2): 3 x SublayerConnection(\n",
            "          (layer_norm): LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x DecoderLayer(\n",
            "        (self_atten): MutiHeadAttention(\n",
            "          (linears): ModuleList(\n",
            "            (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (src_atten): MutiHeadAttention(\n",
            "          (linears): ModuleList(\n",
            "            (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): FeedForward(\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (sub_layers): ModuleList(\n",
            "          (0-2): 3 x SublayerConnection(\n",
            "            (layer_norm): LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm()\n",
            "  )\n",
            "  (source_embed): Sequential(\n",
            "    (0): Embeddings(\n",
            "      (embed): Embedding(1000, 512)\n",
            "    )\n",
            "    (1): PositionEncoding(\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (target_embed): Sequential(\n",
            "    (0): Embeddings(\n",
            "      (embed): Embedding(1000, 512)\n",
            "    )\n",
            "    (1): PositionEncoding(\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (output): Output(\n",
            "    (linear): Linear(in_features=512, out_features=10000, bias=True)\n",
            "  )\n",
            ")\n",
            "tensor([[741, 371, 118, 708],\n",
            "        [188, 561, 850, 678]])\n",
            "tensor([[392, 200, 114, 631, 688,   9],\n",
            "        [ 46, 981, 842, 969, 909, 771]])\n",
            "tensor([[[-10.2569,  -8.4889,  -9.9213,  ...,  -9.7778,  -9.8781, -10.4882],\n",
            "         [ -9.2303,  -8.8870,  -9.5862,  ...,  -9.1377,  -9.4581,  -9.6997],\n",
            "         [ -9.5158,  -8.7031, -10.1818,  ...,  -9.2941,  -9.5683,  -7.8288],\n",
            "         [ -9.6880,  -8.1934,  -9.5567,  ...,  -9.6792,  -9.3150,  -9.7863],\n",
            "         [ -9.0910,  -9.9346,  -9.4170,  ..., -10.0699,  -8.8428,  -9.8578],\n",
            "         [ -8.4835,  -9.8235,  -9.5638,  ...,  -8.9655,  -8.7824,  -8.3957]],\n",
            "\n",
            "        [[ -8.9471, -10.4470,  -9.9694,  ...,  -9.5699,  -9.7027,  -8.9680],\n",
            "         [-10.0043,  -9.7219,  -8.8410,  ...,  -8.6167,  -9.0358,  -8.6152],\n",
            "         [ -9.6001,  -9.4269,  -8.8985,  ..., -10.0138,  -9.6899,  -9.3113],\n",
            "         [ -9.1899,  -9.2839, -10.0245,  ...,  -9.0820,  -9.6827,  -9.5387],\n",
            "         [ -8.7100,  -9.4083,  -9.4998,  ...,  -8.6323,  -9.3938,  -9.4033],\n",
            "         [ -8.9678,  -9.2197,  -9.0077,  ...,  -9.5685,  -8.5473,  -9.2649]]],\n",
            "       grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        }
      ]
    }
  ]
}