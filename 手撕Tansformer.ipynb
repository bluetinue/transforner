{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rymmf30wG2sD"
      ],
      "authorship_tag": "ABX9TyMik5G59DGq2IMJJScJv6xW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bluetinue/transforner/blob/main/%E6%89%8B%E6%92%95Tansformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "# 数学计算工具包\n",
        "import math"
      ],
      "metadata": {
        "id": "ZoK2P_QuHjpk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 输入部分"
      ],
      "metadata": {
        "id": "rymmf30wG2sD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "B1Bt_Sno5B7B",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 词嵌入层\n",
        "class Embed(nn.Module):\n",
        "  def __init__(self,vocab,embed_dim):\n",
        "    super().__init__()\n",
        "    self.vocab = vocab\n",
        "    self.embed_dim = embed_dim\n",
        "    self.lcut = nn.Embedding(self.vocab,self.embed_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.lcut(x) * math.sqrt(self.embed_dim)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 位置编码器\n",
        "class PostionalEncoding(nn.Module):\n",
        "  def __init__(self,d_model,dropout,max_len=60):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    #drop层防止过拟合\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    #[60,512]\n",
        "    pe = torch.zeros(max_len,d_model)\n",
        "    #[60,1]\n",
        "    position = torch.arange(0,max_len,dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "    #定义变化矩阵 [256]\n",
        "    div_term = torch.exp(torch.arange(0,d_model,2) * -(math.log(10000.0) / d_model))\n",
        "\n",
        "    #矩阵相乘 [60,256]\n",
        "    my_matmulres = position * div_term\n",
        "    #按照奇数位*sin，偶数位置 *cos\n",
        "    pe[:,0::2] = torch.sin(my_matmulres)\n",
        "    pe[:,1::2] = torch.cos(my_matmulres)\n",
        "\n",
        "    #[60,512] -->[1,60,512]\n",
        "    pe = pe.unsqueeze(0)\n",
        "\n",
        "    #持久化pe\n",
        "    self.register_buffer(\"pe\",pe)\n",
        "\n",
        "  def forward(self,x):\n",
        "    #对句子长度对应的位置索引进行相加\n",
        "    x = x + self.pe[:,x.size()[1]]\n",
        "    return self.dropout(x)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kWTn_H5c9ZYn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 编码器部分"
      ],
      "metadata": {
        "id": "8ijtOaIYHATt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 自注意力层\n",
        "def attention(q,k,v,mask=None,dropout=None):\n",
        "  d_k = q.size(-1)\n",
        "\n",
        "  #计算权重矩阵q*k的转置\n",
        "  scores = torch.matmul(q,k.transpose(-2,-1)) / math.sqrt(d_k)\n",
        "\n",
        "  #是否对权重进行掩码计算和dropout\n",
        "  if mask is not None:\n",
        "    scores = scores.masked_fill(mask==0,-1e9)\n",
        "\n",
        "  #经过softmax输出权重分布\n",
        "  p_attn = F.softmax(scores,dim=-1)\n",
        "  if dropout is not None:\n",
        "    p_attn = dropout(p_attn)\n",
        "\n",
        "  #返回权重计算结果和权重矩阵\n",
        "  return torch.matmul(p_attn,v),p_attn"
      ],
      "metadata": {
        "id": "Ji7TDBQVHEOZ",
        "cellView": "form"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 多头自注意力层\n",
        "def clones(module,N):\n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,head,embed_dim,dropout_p=0.1):\n",
        "    super().__init__()\n",
        "    #确认维度可以给分\n",
        "    assert embed_dim % head == 0\n",
        "    #计算每个头的维度\n",
        "    self.d_k = embed_dim // head\n",
        "    self.head = head\n",
        "    #随即失活；\n",
        "    self.dropout = nn.Dropout(p=dropout_p)\n",
        "    #定义线性层\n",
        "    self.linears = clones(nn.Linear(embed_dim,embed_dim),4)\n",
        "    #定义atten权重属性\n",
        "    self.atten = None\n",
        "\n",
        "  def forward(self,q,k,v,mask=None):\n",
        "    if mask is not None:\n",
        "      mask = mask.unsqueeze(0)\n",
        "    #计算数据有多少个批次 [2,4,512]\n",
        "    batch_size = q.size(0)\n",
        "    #数据变换，将数据经过线性层组合链接在一起[2,8,4,64]\n",
        "    q,k,v = [model(x).view(batch_size,-1,self.head,self.d_k).transpose(1,2)\n",
        "     for model, x in zip(self.linears,(q,k,v))]\n",
        "    #经过自注意力计算求个各个头之间的自注意力\n",
        "    x,self.attn = attention(q,k,v,mask=mask,dropout=self.dropout)\n",
        "    #数据合并\n",
        "    x = x.transpose(1,2).contiguous().view(batch_size,-1,self.head*self.d_k)\n",
        "    #返回线性层输出\n",
        "    return self.linears[-1](x)"
      ],
      "metadata": {
        "id": "7FbXcfkb0z9X",
        "cellView": "form"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 前馈连接层\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,d_model,d_ff,dropout=0.1):\n",
        "    super().__init__()\n",
        "    #定义两个线性层让数据走一遍，不改变原有的形状\n",
        "    self.w_1 = nn.Linear(d_model,d_ff)\n",
        "    self.w_2 = nn.Linear(d_ff,d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    #第一层经过relu激活函数，在经过dropout，在进第二层\n",
        "    return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "S_FKdzTfMWYa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 规范化层\n",
        "class Norm(nn.Module):\n",
        "  def __init__(self,features,eps=1e-6):\n",
        "    super().__init__()\n",
        "    #权重\n",
        "    self.a1 = nn.Parameter(torch.ones(features))\n",
        "    #偏置\n",
        "    self.w1 = nn.Parameter(torch.zeros(features))\n",
        "\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean = x.mean(-1,keepdim=True)\n",
        "    std = x.std(-1,keepdim=True)\n",
        "    return self.a1 * (x-mean) / (std+self.eps) + self.w1\n",
        ""
      ],
      "metadata": {
        "cellView": "form",
        "id": "bFqJLSIK9nj8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 残差连接层"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lyUMJ0-o_LI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 测试\n",
        "vocab = 1000\n",
        "embed_dim = 512\n",
        "embedding = Embed(vocab,embed_dim)\n",
        "posEncodeing = PostionalEncoding(embed_dim,0.1)\n",
        "x = torch.tensor([[1,2,3,4],[40,50,60,70]])\n",
        "embed_result = embedding(x)\n",
        "position_result = posEncodeing(embed_result)\n",
        "mask = torch.zeros(2,4,4)\n",
        "q = k = v = position_result\n",
        "# #不带mask\n",
        "# attn,p_attn = attention(q,k,v)\n",
        "# print(attn.shape)\n",
        "# print(p_attn.shape)\n",
        "# #带mask\n",
        "# attn,p_attn = attention(q,k,v,mask)\n",
        "# my_mutilhead = MultiHeadAttention(8,embed_dim)\n",
        "# print(my_mutilhead)\n",
        "my_mutilhead = MultiHeadAttention(8,embed_dim)\n",
        "att = my_mutilhead(q,k,v)\n",
        "\n",
        "print(att.shape)\n",
        "\n",
        "my_ff = FeedForward(embed_dim,2048)\n",
        "result = my_ff(att)\n",
        "my_norm = Norm(embed_dim)\n",
        "print(my_norm(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaA8BlLqIyYi",
        "outputId": "d64dfc08-e9b0-4fa7-b15b-a369b9b5699d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 512])\n",
            "tensor([[[-0.3992, -1.8512,  1.0561,  ..., -0.9457, -0.8954,  0.1163],\n",
            "         [ 0.9359, -1.7135,  1.3636,  ..., -0.9879,  0.6600, -1.2491],\n",
            "         [-1.1123, -1.3216, -0.0183,  ..., -1.6209, -0.2633, -0.7066],\n",
            "         [-0.8027, -0.1744,  0.8086,  ..., -0.0366,  0.1630, -1.8758]],\n",
            "\n",
            "        [[ 1.1042, -1.0164, -0.4056,  ...,  0.3457, -0.7491,  0.7653],\n",
            "         [ 0.6335, -1.4128,  0.3975,  ..., -1.3829, -0.2537, -0.4565],\n",
            "         [ 0.8767, -0.1942, -0.6632,  ..., -1.7068,  0.1544, -0.0264],\n",
            "         [ 0.1903, -1.0553,  1.1933,  ..., -0.6232,  0.2391, -0.8882]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "47tCBgzQJ9Vm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}