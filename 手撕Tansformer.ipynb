{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rymmf30wG2sD",
        "8ijtOaIYHATt"
      ],
      "authorship_tag": "ABX9TyOCsZ631hUc/IxODbkh1E/r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bluetinue/transforner/blob/main/%E6%89%8B%E6%92%95Tansformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "# 数学计算工具包\n",
        "import math"
      ],
      "metadata": {
        "id": "ZoK2P_QuHjpk"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 输入部分"
      ],
      "metadata": {
        "id": "rymmf30wG2sD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "B1Bt_Sno5B7B",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 词嵌入层\n",
        "class Embed(nn.Module):\n",
        "  def __init__(self,vocab,embed_dim):\n",
        "    super().__init__()\n",
        "    self.vocab = vocab\n",
        "    self.embed_dim = embed_dim\n",
        "    self.lcut = nn.Embedding(self.vocab,self.embed_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.lcut(x) * math.sqrt(self.embed_dim)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 位置编码器\n",
        "class PostionalEncoding(nn.Module):\n",
        "  def __init__(self,d_model,dropout,max_len=60):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    #drop层防止过拟合\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    #[60,512]\n",
        "    pe = torch.zeros(max_len,d_model)\n",
        "    #[60,1]\n",
        "    position = torch.arange(0,max_len,dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "    #定义变化矩阵 [256]\n",
        "    div_term = torch.exp(torch.arange(0,d_model,2) * -(math.log(10000.0) / d_model))\n",
        "\n",
        "    #矩阵相乘 [60,256]\n",
        "    my_matmulres = position * div_term\n",
        "    #按照奇数位*sin，偶数位置 *cos\n",
        "    pe[:,0::2] = torch.sin(my_matmulres)\n",
        "    pe[:,1::2] = torch.cos(my_matmulres)\n",
        "\n",
        "    #[60,512] -->[1,60,512]\n",
        "    pe = pe.unsqueeze(0)\n",
        "\n",
        "    #持久化pe\n",
        "    self.register_buffer(\"pe\",pe)\n",
        "\n",
        "  def forward(self,x):\n",
        "    #对句子长度对应的位置索引进行相加\n",
        "    x = x + self.pe[:,x.size()[1]]\n",
        "    return self.dropout(x)"
      ],
      "metadata": {
        "id": "kWTn_H5c9ZYn"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 编码器部分"
      ],
      "metadata": {
        "id": "8ijtOaIYHATt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 自注意力层\n",
        "def attention(q,k,v,mask=None,dropout=None):\n",
        "  d_k = q.size(-1)\n",
        "\n",
        "  #计算权重矩阵q*k的转置\n",
        "  scores = torch.matmul(q,k.transpose(-2,-1)) / math.sqrt(d_k)\n",
        "\n",
        "  #是否对权重进行掩码计算和dropout\n",
        "  if mask is not None:\n",
        "    scores = scores.masked_fill(mask==0,-1e9)\n",
        "\n",
        "  #经过softmax输出权重分布\n",
        "  p_attn = F.softmax(scores,dim=-1)\n",
        "  if dropout is not None:\n",
        "    p_attn = dropout(p_attn)\n",
        "\n",
        "  #返回权重计算结果和权重矩阵\n",
        "  return torch.matmul(p_attn,v),p_attn"
      ],
      "metadata": {
        "id": "Ji7TDBQVHEOZ",
        "cellView": "form"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 多头自注意力层\n",
        "def clones(module,N):\n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,head,embed_dim,dropout_p=0.1):\n",
        "    super().__init__()\n",
        "    #确认维度可以给分\n",
        "    assert embed_dim % head == 0\n",
        "    #计算每个头的维度\n",
        "    self.d_k = embed_dim // head\n",
        "    self.head = head\n",
        "    #随即失活；\n",
        "    self.dropout = nn.Dropout(p=dropout_p)\n",
        "    #定义线性层\n",
        "    self.linears = clones(nn.Linear(embed_dim,embed_dim),4)\n",
        "    #定义atten权重属性\n",
        "    self.atten = None\n",
        "\n",
        "  def forward(self,q,k,v,mask=None):\n",
        "    if mask is not None:\n",
        "      mask = mask.unsqueeze(0)\n",
        "    #计算数据有多少个批次 [2,4,512]\n",
        "    batch_size = q.size(0)\n",
        "    #数据变换，将数据经过线性层组合链接在一起[2,8,4,64]\n",
        "    q,k,v = [model(x).view(batch_size,-1,self.head,self.d_k).transpose(1,2)\n",
        "     for model, x in zip(self.linears,(q,k,v))]\n",
        "    #经过自注意力计算求个各个头之间的自注意力\n",
        "    x,self.attn = attention(q,k,v,mask=mask,dropout=self.dropout)\n",
        "    #数据合并\n",
        "    x = x.transpose(1,2).contiguous().view(batch_size,-1,self.head*self.d_k)\n",
        "    #返回线性层输出\n",
        "    return self.linears[-1](x)"
      ],
      "metadata": {
        "id": "7FbXcfkb0z9X",
        "cellView": "form"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 前馈连接层\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,d_model,d_ff,dropout=0.1):\n",
        "    super().__init__()\n",
        "    #定义两个线性层让数据走一遍，不改变原有的形状\n",
        "    self.w_1 = nn.Linear(d_model,d_ff)\n",
        "    self.w_2 = nn.Linear(d_ff,d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    #第一层经过relu激活函数，在经过dropout，在进第二层\n",
        "    return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "S_FKdzTfMWYa"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 规范化层\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self,features,eps=1e-6):\n",
        "    super().__init__()\n",
        "    #权重\n",
        "    self.a1 = nn.Parameter(torch.ones(features))\n",
        "    #偏置\n",
        "    self.w1 = nn.Parameter(torch.zeros(features))\n",
        "\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean = x.mean(-1,keepdim=True)\n",
        "    std = x.std(-1,keepdim=True)\n",
        "    return self.a1 * (x-mean) / (std+self.eps) + self.w1\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bFqJLSIK9nj8"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 拼接两个子层起来形成编码器层\n",
        "\n",
        "# 子层连接结构 子层(前馈全连接层 或者 注意力机制层)+ norm层 + 残差连接\n",
        "# SublayerConnection实现思路分析\n",
        "# 1 init函数  (self, size, dropout=0.1):\n",
        "# 定义self.norm层 self.dropout层, 其中LayerNorm(size)\n",
        "# 2 forward(self, x, sublayer) 返回+以后的结果\n",
        "# 数据self.norm() -> sublayer()->self.dropout() + x\n",
        "class SublayerConnection(nn.Module):\n",
        "  def __init__(self,size,dropout):\n",
        "    super().__init__()\n",
        "    #定义norm和dropout层\n",
        "    self.norm = LayerNorm(size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x,sublayer):\n",
        "    myres = x + self.dropout(self.norm(sublayer(x)))\n",
        "    return myres\n",
        "\n",
        "#编码器层\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self,size,self_atten,feed_forward,dropout):\n",
        "    super().__init__()\n",
        "    self.self_atten = self_atten\n",
        "    self.feed_forward = feed_forward\n",
        "    self.size = size\n",
        "    self.sublayers = clones(SublayerConnection(size,dropout),2)\n",
        "\n",
        "  def forward(self,x,mask):\n",
        "    x = self.sublayers[0](x,lambda a:self.self_atten(a,a,a,mask))\n",
        "    x = self.sublayers[1](x,self.feed_forward)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Jd-hrc5HDJ1s",
        "cellView": "form"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 拼接6个编码层起来形成编码器\n",
        "#编码器\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self,layer,N):\n",
        "    super().__init__()\n",
        "    self.layer = layer\n",
        "    self.N = N\n",
        "\n",
        "    self.layers = clones(layer,N)\n",
        "    #实例化规范化层\n",
        "    self.norm = LayerNorm(layer.size)\n",
        "\n",
        "  def forward(self,x,mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x,mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gjdsQrZzSOa0"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 测试\n",
        "embed_dim = 512\n",
        "vocab = 1000\n",
        "x = torch.tensor([[1,2,3,4],[40,50,60,70]])\n",
        "embedding = Embed(vocab,embed_dim)\n",
        "embr = embedding(x)\n",
        "dropout = 0.1\n",
        "x = embr\n",
        "pe = PostionalEncoding(embed_dim,dropout)\n",
        "pe_result = pe(x)\n",
        "\n",
        "x = pe_result\n",
        "mask = torch.zeros(8,4,4)\n",
        "self_attn = MultiHeadAttention(8,embed_dim)\n",
        "feed_forward = FeedForward(embed_dim,2048)\n",
        "encoder_layer = EncoderLayer(embed_dim,self_attn,feed_forward,dropout)\n",
        "\n",
        "encoder = Encoder(encoder_layer,6)\n",
        "encoder_result = encoder(x,mask)\n",
        "encoder_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaA8BlLqIyYi",
        "outputId": "0da6df4c-e94f-45b7-fa3d-0795ec36feeb"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0453,  0.0159, -1.7737,  ...,  0.6770, -0.3944,  0.0782],\n",
              "         [ 0.2380, -0.0404,  1.1892,  ...,  2.2801, -0.0617,  0.6955],\n",
              "         [ 1.9359, -0.9223,  0.4601,  ...,  1.2048,  2.2784, -1.1281],\n",
              "         [ 1.3250, -1.0286,  2.1739,  ...,  1.2817, -0.9785,  1.4144]],\n",
              "\n",
              "        [[-0.0831,  0.1433, -2.2373,  ...,  1.6918, -0.0959,  1.7624],\n",
              "         [ 0.0648,  0.4727, -1.0945,  ..., -1.3974, -1.0920, -0.1429],\n",
              "         [ 1.9970,  0.6371,  1.1651,  ...,  0.0591,  0.9907, -0.6546],\n",
              "         [ 0.0590, -1.4551,  2.1309,  ..., -0.3201,  0.0919,  0.0973]]],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 解码器层\n"
      ],
      "metadata": {
        "id": "KOIHCfDXC-h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 解码器层\n",
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self,size,self_attn,src_attn,feed_forward,dropout):\n",
        "    super().__init__()\n",
        "    self.size = size\n",
        "    self.self_attn = self_attn\n",
        "    self.src_attn = src_attn\n",
        "    self.feed_forward = feed_forward\n",
        "    self.sublayers = clones(SublayerConnection(size,dropout),3)\n",
        "\n",
        "  def forward(self,y,encoder_output,source_mask,target_mask):\n",
        "    y1 = self.sublayers[0](y,lambda a:self.self_attn(a,a,a,target_mask))\n",
        "    y2 = self.sublayers[1](y1,lambda a:self.src_attn(a,encoder_output,encoder_output,source_mask))\n",
        "    y3 = self.sublayers[2](y2,self.feed_forward)\n",
        "    return y3"
      ],
      "metadata": {
        "id": "ccxcaHYJDFDk"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 解码器由若干个解码器层堆叠而成\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self,layer,N):\n",
        "    super().__init__()\n",
        "    self.layer = layer\n",
        "    self.N = N\n",
        "\n",
        "    self.layers = clones(layer,N)\n",
        "    #实例化规范化层\n",
        "    self.norm = LayerNorm(layer.size)\n",
        "\n",
        "  def forward(self,y,encoder_output,source_mask,target_mask):\n",
        "    for layer in self.layers:\n",
        "      y = layer(y,encoder_output,source_mask,target_mask)\n",
        "    return self.norm(y)"
      ],
      "metadata": {
        "id": "gt4kE7B1hdov"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 测试\n",
        "y0 = torch.tensor([[2, 4, 10, 29, 67, 89],\n",
        "          [34, 56, 78, 20, 19, 6]])\n",
        "embed_y = embedding(y0)\n",
        "position_y = pe(embed_y)\n",
        "\n",
        "muti_head_atten = MultiHeadAttention(8,embed_dim)\n",
        "self_attn = copy.deepcopy(muti_head_atten)\n",
        "src_atten = copy.deepcopy(muti_head_atten)\n",
        "ff = FeedForward(embed_dim,2048)\n",
        "decoder_layer = DecoderLayer(embed_dim,self_attn,src_atten,feed_forward,dropout)\n",
        "decoder = Decoder(decoder_layer,6)\n",
        "#掩码张量\n",
        "source_mask = torch.zeros(8,6,4)\n",
        "target_mask = torch.zeros(8,6,6)\n",
        "\n",
        "result = decoder(y=position_y,encoder_output=encoder_result,source_mask=source_mask,target_mask=target_mask)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFpPYJ2Ee5MF",
        "outputId": "117b6f6c-6196-4892-d165-01e7332efc16"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.4328, -0.1102,  1.3320,  ...,  2.5533,  0.3870,  0.3272],\n",
            "         [ 0.7533, -1.0068,  2.4373,  ...,  1.4720, -0.5158,  1.1373],\n",
            "         [ 0.3494, -0.7216,  1.2888,  ...,  1.7521, -1.0551,  0.7279],\n",
            "         [-0.7565, -0.3076,  0.2183,  ..., -1.3209, -0.9969,  3.3321],\n",
            "         [ 1.2997,  1.2106, -0.5191,  ...,  0.8102,  0.4786, -1.2705],\n",
            "         [-0.9406,  0.3187, -0.1868,  ...,  0.1667,  0.7771, -0.5894]],\n",
            "\n",
            "        [[-0.3276,  0.4991,  0.3318,  ..., -1.3001,  0.1996, -0.1494],\n",
            "         [ 0.7510,  0.1927, -0.5018,  ..., -1.6089, -1.2535,  0.8717],\n",
            "         [-1.6750, -0.0944,  1.5664,  ..., -0.6827, -0.2770, -0.0150],\n",
            "         [-0.0236,  0.9054,  0.3642,  ..., -0.0687,  0.8433, -1.2040],\n",
            "         [-0.8430,  1.3097,  0.3392,  ..., -0.2594,  0.3848,  0.3626],\n",
            "         [-1.8906, -0.9670, -0.4066,  ..., -0.0601,  0.1691, -0.2599]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    }
  ]
}