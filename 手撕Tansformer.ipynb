{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVKP5AYLOfm4vfob0iEZpU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bluetinue/transforner/blob/main/%E6%89%8B%E6%92%95Tansformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "# 数学计算工具包\n",
        "import math"
      ],
      "metadata": {
        "id": "ZoK2P_QuHjpk"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 输入部分"
      ],
      "metadata": {
        "id": "rymmf30wG2sD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "B1Bt_Sno5B7B"
      },
      "outputs": [],
      "source": [
        "#@title 词嵌入层\n",
        "class Embed(nn.Module):\n",
        "  def __init__(self,vocab,embed_dim):\n",
        "    super().__init__()\n",
        "    self.vocab = vocab\n",
        "    self.embed_dim = embed_dim\n",
        "    self.lcut = nn.Embedding(self.vocab,self.embed_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.lcut(x) * math.sqrt(self.embed_dim)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 位置编码器\n",
        "class PostionalEncoding(nn.Module):\n",
        "  def __init__(self,d_model,dropout,max_len=60):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    #drop层防止过拟合\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    #[60,512]\n",
        "    pe = torch.zeros(max_len,d_model)\n",
        "    #[60,1]\n",
        "    position = torch.arange(0,max_len,dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "    #定义变化矩阵 [256]\n",
        "    div_term = torch.exp(torch.arange(0,d_model,2) * -(math.log(10000.0) / d_model))\n",
        "\n",
        "    #矩阵相乘 [60,256]\n",
        "    my_matmulres = position * div_term\n",
        "    #按照奇数位*sin，偶数位置 *cos\n",
        "    pe[:,0::2] = torch.sin(my_matmulres)\n",
        "    pe[:,1::2] = torch.cos(my_matmulres)\n",
        "\n",
        "    #[60,512] -->[1,60,512]\n",
        "    pe = pe.unsqueeze(0)\n",
        "\n",
        "    #持久化pe\n",
        "    self.register_buffer(\"pe\",pe)\n",
        "\n",
        "  def forward(self,x):\n",
        "    #对句子长度对应的位置索引进行相加\n",
        "    x = x + self.pe[:,x.size()[1]]\n",
        "    return self.dropout(x)"
      ],
      "metadata": {
        "id": "kWTn_H5c9ZYn"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 编码器部分"
      ],
      "metadata": {
        "id": "8ijtOaIYHATt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 自注意力层\n",
        "def attention(q,k,v,mask=None,dropout=None):\n",
        "  d_k = q.size(-1)\n",
        "\n",
        "  #计算权重矩阵q*k的转置\n",
        "  scores = torch.matmul(q,k.transpose(-2,-1)) / math.sqrt(d_k)\n",
        "\n",
        "  #是否对权重进行掩码计算和dropout\n",
        "  if mask is not None:\n",
        "    scores = scores.masked_fill(mask==0,-1e9)\n",
        "\n",
        "  #经过softmax输出权重分布\n",
        "  p_attn = F.softmax(scores,dim=-1)\n",
        "  if dropout is not None:\n",
        "    p_attn = dropout(p_attn)\n",
        "\n",
        "  #返回权重计算结果和权重矩阵\n",
        "  return torch.matmul(p_attn,v),p_attn"
      ],
      "metadata": {
        "id": "Ji7TDBQVHEOZ"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 多头自注意力层\n",
        "def clones(module,N):\n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,head,embed_dim,dropout_p=0.1):\n",
        "    super().__init__()\n",
        "    #确认维度可以给分\n",
        "    assert embed_dim % head == 0\n",
        "    #计算每个头的维度\n",
        "    self.d_k = embed_dim // head\n",
        "    self.head = head\n",
        "    #随即失活；\n",
        "    self.dropout = nn.Dropout(p=dropout_p)\n",
        "    #定义线性层\n",
        "    self.linears = clones(nn.Linear(embed_dim,embed_dim),4)\n",
        "    #定义atten权重属性\n",
        "    self.atten = None\n",
        "\n",
        "  def forward(self,q,k,v,mask=None):\n",
        "    if mask is not None:\n",
        "      mask = mask.unsqueeze(0)\n",
        "    #计算数据有多少个批次 [2,4,512]\n",
        "    batch_size = q.size(0)\n",
        "    #数据变换，将数据经过线性层组合链接在一起[2,8,4,64]\n",
        "    q,k,v = [model(x).view(batch_size,-1,self.head,self.d_k).transpose(1,2)\n",
        "     for model, x in zip(self.linears,(q,k,v))]\n",
        "    #经过自注意力计算求个各个头之间的自注意力\n",
        "    x,self.attn = attention(q,k,v,mask=mask,dropout=self.dropout)\n",
        "    #数据合并\n",
        "    x = x.transpose(1,2).contiguous().view(batch_size,-1,self.head*self.d_k)\n",
        "    #返回线性层输出\n",
        "    return self.linears[-1](x)"
      ],
      "metadata": {
        "id": "7FbXcfkb0z9X"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 前馈连接层\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,d_model,d_ff,dropout=0.1):\n",
        "    super().__init__()\n",
        "    #定义两个线性层让数据走一遍，不改变原有的形状\n",
        "    self.w_1 = nn.Linear(d_model,d_ff)\n",
        "    self.w_2 = nn.Linear(d_ff,d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    #第一层经过relu激活函数，在经过dropout，在进第二层\n",
        "    return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ],
      "metadata": {
        "id": "S_FKdzTfMWYa"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 规范化层\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self,features,eps=1e-6):\n",
        "    super().__init__()\n",
        "    #权重\n",
        "    self.a1 = nn.Parameter(torch.ones(features))\n",
        "    #偏置\n",
        "    self.w1 = nn.Parameter(torch.zeros(features))\n",
        "\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean = x.mean(-1,keepdim=True)\n",
        "    std = x.std(-1,keepdim=True)\n",
        "    return self.a1 * (x-mean) / (std+self.eps) + self.w1\n"
      ],
      "metadata": {
        "id": "bFqJLSIK9nj8"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 拼接两个子层起来形成编码器层\n",
        "\n",
        "# 子层连接结构 子层(前馈全连接层 或者 注意力机制层)+ norm层 + 残差连接\n",
        "# SublayerConnection实现思路分析\n",
        "# 1 init函数  (self, size, dropout=0.1):\n",
        "# 定义self.norm层 self.dropout层, 其中LayerNorm(size)\n",
        "# 2 forward(self, x, sublayer) 返回+以后的结果\n",
        "# 数据self.norm() -> sublayer()->self.dropout() + x\n",
        "class SublayerConnection(nn.Module):\n",
        "  def __init__(self,size,dropout):\n",
        "    super().__init__()\n",
        "    #定义norm和dropout层\n",
        "    self.norm = LayerNorm(size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x,sublayer):\n",
        "    myres = x + self.dropout(self.norm(sublayer(x)))\n",
        "    return myres\n",
        "\n",
        "#编码器层\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self,size,self_atten,feed_forward,dropout):\n",
        "    super().__init__()\n",
        "    self.self_atten = self_atten\n",
        "    self.feed_forward = feed_forward\n",
        "    self.size = size\n",
        "    self.sublayers = clones(SublayerConnection(size,dropout),2)\n",
        "\n",
        "  def forward(self,x,mask):\n",
        "    x = self.sublayers[0](x,lambda a:self.self_atten(a,a,a,mask))\n",
        "    x = self.sublayers[1](x,self.feed_forward)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Jd-hrc5HDJ1s",
        "cellView": "form"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 拼接6个编码层起来形成编码器\n",
        "#编码器\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self,layer,N):\n",
        "    super().__init__()\n",
        "    self.layer = layer\n",
        "    self.N = N\n",
        "\n",
        "    self.layers = clones(layer,N)\n",
        "    #实例化规范化层\n",
        "    self.norm = LayerNorm(layer.size)\n",
        "\n",
        "  def forward(self,x,mask):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x,mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gjdsQrZzSOa0"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 测试\n",
        "embed_dim = 512\n",
        "vocab = 1000\n",
        "x = torch.tensor([[1,2,3,4],[40,50,60,70]])"
      ],
      "metadata": {
        "id": "xaA8BlLqIyYi"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 解码器层\n"
      ],
      "metadata": {
        "id": "KOIHCfDXC-h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 解码器层\n",
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self,size,self_attn,src_attn,feed_forward,dropout):\n",
        "    super().__init__()\n",
        "    self.size = size\n",
        "    self.self_attn = self_attn\n",
        "    self.src_attn = src_attn\n",
        "    self.feed_forward = feed_forward\n",
        "    self.sublayers = clones(SublayerConnection(size,dropout),3)\n",
        "\n",
        "  def forward(self,y,encoder_output,source_mask,target_mask):\n",
        "    y1 = self.sublayers[0](y,lambda a:self.self_attn(a,a,a,target_mask))\n",
        "    y2 = self.sublayers[1](y1,lambda a:self.src_attn(a,encoder_output,encoder_output,source_mask))\n",
        "    y3 = self.sublayers[2](y2,self.feed_forward)\n",
        "    return y3"
      ],
      "metadata": {
        "id": "ccxcaHYJDFDk"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 解码器由若干个解码器层堆叠而成\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self,layer,N):\n",
        "    super().__init__()\n",
        "    self.layer = layer\n",
        "    self.N = N\n",
        "\n",
        "    self.layers = clones(layer,N)\n",
        "    #实例化规范化层\n",
        "    self.norm = LayerNorm(layer.size)\n",
        "\n",
        "  def forward(self,y,encoder_output,source_mask,target_mask):\n",
        "    for layer in self.layers:\n",
        "      y = layer(y,encoder_output,source_mask,target_mask)\n",
        "    return self.norm(y)"
      ],
      "metadata": {
        "id": "gt4kE7B1hdov"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 输出层"
      ],
      "metadata": {
        "id": "Zfu_o2XJilYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 构建线性层和softmax层作为输出层# 解码器类 Generator 实现思路分析\n",
        "# init函数 (self, d_model, vocab_size)\n",
        "# 定义线性层self.project\n",
        "# forward函数 (self, x)\n",
        "# 数据 F.log_softmax(self.project(x), dim=-1)\n",
        "class Output(nn.Module):\n",
        "  def __init__(self,d_model,vocab_size):\n",
        "    super().__init__()\n",
        "    #定义线性层\n",
        "    self.linear = nn.Linear(d_model,vocab_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return F.log_softmax(self.linear(x),dim=-1)"
      ],
      "metadata": {
        "id": "87rNCYDq0Zbn"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 将多个层构建成模型类"
      ],
      "metadata": {
        "id": "Uh5Lu1jq2YkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 编码器-解码器\n",
        "#思路\n",
        "# 编码解码内部函数类 EncoderDecoder 实现分析\n",
        "# init函数 (self, encoder, decoder, source_embed, target_embed, generator)\n",
        "# 5个成员属性赋值 encoder 编码器对象 decoder 解码器对象 source_embed source端词嵌入层对象\n",
        "# target_embed target端词嵌入层对象 generator 输出层对象\n",
        "# forward函数 (self, source,  target, source_mask, target_mask)\n",
        "# 1 编码 s.encoder(self.src_embed(source), source_mask)\n",
        "# 2 解码 s.decoder(self.target_embed(target), memory, source_mask, target_mask)\n",
        "# 3 输出 s.output()\n",
        "\n",
        "class EncoderDecoder(nn.Module):\n",
        "  def __init__(self,encoder,decoder,x_embed,y_embed,output):\n",
        "    super().__init__()\n",
        "    #encoder 编码器\n",
        "    #decoder 解码器\n",
        "    #x_embed 源文本词嵌入层及位置索引编码器\n",
        "    #y_embed 目标文本词嵌入层及位置索引编码器\n",
        "    #output 输出层\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.x_embed = x_embed\n",
        "    self.y_embed = y_embed\n",
        "    self.output = output\n",
        "\n",
        "  def forward(self,x,y,x_mask,y_mask):\n",
        "    return self.output(self.decoder(self.encode(x,x_mask),x_mask,y,y_mask))\n",
        "\n",
        "  def encode(self,x,x_mask):\n",
        "    return self.encoder(self.x_embed(x),x_mask)\n",
        "\n",
        "  def decode(self,y,y_mask):\n",
        "    return self.decoder(self.y_embed(y),y_mask)\n"
      ],
      "metadata": {
        "id": "J8rMdo5s2ew_"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Tansformer模型构建过程\n",
        "# make_model函数实现思路分析\n",
        "# 函数原型(x_vocab, y_vocab, N=6, d_model=512, d_ff=2048, head=8, dropout=0.1)\n",
        "# 实例化多头注意力层对象 attn\n",
        "# 实例化前馈全连接对象ff\n",
        "# 实例化位置编码器对象position\n",
        "# 构建 EncoderDecoder对象(Encoder对象, Decoder对象,)\n",
        "# x端输入部分nn.Sequential(),\n",
        "# y端输入部分nn.Sequential(),\n",
        "# 线性层输出Generator)\n",
        "# 对模型参数初始化 nn.init.xavier_uniform_(p)\n",
        "# 注意使用 c = copy.deepcopy\n",
        "# 返回model\n",
        "\n",
        "def MakeTransformer(x_vocab, y_vocab, N=6, d_model=512, d_ff=2048, head=8, dropout=0.1):\n",
        "  c = copy.deepcopy\n",
        "  #实例化多头注意力层对象 attn\n",
        "  attn = MultiHeadAttention(head=head,embed_dim=512)\n",
        "  # 实例化前馈全连接对象ff\n",
        "  ff = FeedForward(d_model,d_ff)\n",
        "  # 实例化位置编码器对象position\n",
        "  position = PostionalEncoding(d_model,dropout)\n",
        "  #构建 EncoderDecoder对象\n",
        "  model = EncoderDecoder(\n",
        "      #编码器对象\n",
        "      Encoder(EncoderLayer(d_model,c(attn),c(ff),dropout),N),\n",
        "      #解码器对象\n",
        "      Decoder(DecoderLayer(d_model,c(attn),c(attn),c(ff),dropout),N),\n",
        "      #x端输入部分nn.Sequential()\n",
        "      nn.Sequential(Embed(x_vocab,d_model),c(position)),\n",
        "      #y端输入部分nn.Sequential()\n",
        "      nn.Sequential(Embed(y_vocab,d_model),c(position)),\n",
        "      #线性层输出Generator\n",
        "      Output(d_model,y_vocab))\n",
        "\n",
        "  for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "vpc-GJ-c6JJa"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mytransformer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkNhbbpuCAPE",
        "outputId": "189920b7-13dc-44cb-b4bd-9399fc314b09"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EncoderDecoder(\n",
            "  (encoder): Encoder(\n",
            "    (layer): EncoderLayer(\n",
            "      (self_atten): MultiHeadAttention(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linears): ModuleList(\n",
            "          (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (sublayers): ModuleList(\n",
            "        (0-1): 2 x SublayerConnection(\n",
            "          (norm): LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x EncoderLayer(\n",
            "        (self_atten): MultiHeadAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linears): ModuleList(\n",
            "            (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (feed_forward): FeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (sublayers): ModuleList(\n",
            "          (0-1): 2 x SublayerConnection(\n",
            "            (norm): LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm()\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (layer): DecoderLayer(\n",
            "      (self_attn): MultiHeadAttention(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linears): ModuleList(\n",
            "          (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (src_attn): MultiHeadAttention(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linears): ModuleList(\n",
            "          (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (sublayers): ModuleList(\n",
            "        (0-2): 3 x SublayerConnection(\n",
            "          (norm): LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x DecoderLayer(\n",
            "        (self_attn): MultiHeadAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linears): ModuleList(\n",
            "            (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (src_attn): MultiHeadAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linears): ModuleList(\n",
            "            (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (feed_forward): FeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (sublayers): ModuleList(\n",
            "          (0-2): 3 x SublayerConnection(\n",
            "            (norm): LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm()\n",
            "  )\n",
            "  (x_embed): Sequential(\n",
            "    (0): Embed(\n",
            "      (lcut): Embedding(500, 512)\n",
            "    )\n",
            "    (1): PostionalEncoding(\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (y_embed): Sequential(\n",
            "    (0): Embed(\n",
            "      (lcut): Embedding(1000, 512)\n",
            "    )\n",
            "    (1): PostionalEncoding(\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (output): Output(\n",
            "    (linear): Linear(in_features=512, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    }
  ]
}